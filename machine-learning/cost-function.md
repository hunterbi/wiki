# cost function
* cross entropy  

   Cross-entropy log loss measures (classification model) output is a probability value between 0 and 1.    
   ![Screenshot](https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)      

* binary cost function   

   cost(hθ, (x),y)     
   = -ylog( hθ(x) ) -  (1-y)log( 1- hθ(x) )         
   =  -log( hθ(x) ) -       log( 1- hθ(x) )  
   
   Then     
   ![Screenshot](http://www.holehouse.org/mlclass/06_Logistic_Regression_files/Image%20[16].png)    






   
