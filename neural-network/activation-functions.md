# activation functions
 
 ![Screenshot](https://cdn-images-1.medium.com/max/1600/1*p_hyqAtyI8pbt2kEl6siOQ.png)    
 

# difference

#### Sigmoids
 * saturate and kill gradients. Sigmoid outputs are not zero-centered.

#### tanh
 * saturate and kill gradients. Sigmoid outputs are zero-centered.
 Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.

#### relu
 * Does not saturate (in +ve region) 
 * Computationally, it is very efficient

# function
#### softmax Function 
![Screenshot](https://wikimedia.org/api/rest_v1/media/math/render/svg/46c32a5089726d673c30a0abfda7b35ecf0fe3ca)

* Logistic function

![Screenshot](https://wikimedia.org/api/rest_v1/media/math/render/svg/faaa0c014ae28ac67db5c49b3f3e8b08415a3f2b)

![Screenshot](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png)
  


#### Hyperbolic tangent 
![Screenshot](https://wikimedia.org/api/rest_v1/media/math/render/svg/4f1b5f1173b93d23c64a0d3508028f8649a5a14e)    
 
![Screenshot](https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Sinh_cosh_tanh.svg/600px-Sinh_cosh_tanh.svg.png)


