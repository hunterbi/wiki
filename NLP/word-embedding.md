
# word embedding
word representation, words or phrases from the vocabulary are mapped to vectors of real numbers 



####  word embedding algorithm
* unsupervised learning     
* obtain vector representations for words   


1. continuous bag-of-words 


2. skip-gram architectures  

![Sreenshot](https://skymind.ai/images/wiki/word2vec_diagrams.png)




#### pretained model    
[GloVe](https://nlp.stanford.edu/projects/glove/)       
[word2vec](https://code.google.com/archive/p/word2vec/)   


# reference
[1] [A Closer Look at Skip-gram Modelling ](https://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)   
 
